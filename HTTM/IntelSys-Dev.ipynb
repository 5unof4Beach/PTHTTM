{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Linear Regression\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Calculate the mean of X and y\n",
    "        X_mean = np.mean(X)\n",
    "        y_mean = np.mean(y)\n",
    "        \n",
    "        # Calculate the variance of X and y\n",
    "        X_var = np.var(X)\n",
    "        y_var = np.var(y)\n",
    "        \n",
    "        # Calculate the covariance of X and y\n",
    "        X_y_cov = np.cov(X, y)[0][1]\n",
    "        \n",
    "        # Calculate the weights w and b\n",
    "        self.w = X_y_cov / X_var\n",
    "        self.b = y_mean - self.w * X_mean\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.w * X + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Logistic Regression\n",
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "  return x * (1 - x)\n",
    "\n",
    "# Loss function\n",
    "def log_loss(y_true, y_pred):\n",
    "  return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Input data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Initialize weights and bias\n",
    "weights = np.random.rand(2, 1)\n",
    "bias = np.random.rand(1)\n",
    "\n",
    "# Set learning rate and number of epochs\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10000\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "  # Forward pass\n",
    "  z = np.dot(X, weights) + bias\n",
    "  y_pred = sigmoid(z)\n",
    "\n",
    "  # Compute loss\n",
    "  loss = log_loss(y, y_pred)\n",
    "  loss = np.mean(loss)\n",
    "\n",
    "  # Backward pass\n",
    "  derivative_loss_y_pred = y_pred - y\n",
    "  derivative_y_pred_z = sigmoid_derivative(y_pred)\n",
    "  derivative_loss_z = derivative_loss_y_pred * derivative_y_pred_z\n",
    "  derivative_z_weights = X\n",
    "  derivative_loss_weights = np.dot(derivative_z_weights.T, derivative_loss_z) / len(X)\n",
    "  derivative_loss_bias = np.mean(derivative_loss_z)\n",
    "\n",
    "  # Update weights and bias\n",
    "  weights -= learning_rate * derivative_loss_weights\n",
    "  bias -= learning_rate * derivative_loss_bias\n",
    "\n",
    "# Print the final weights and bias\n",
    "print(weights)\n",
    "print(bias)\n",
    "\n",
    "# Test the model\n",
    "test_input = np.array([[1, 1]])\n",
    "test_output = sigmoid(np.dot(test_input, weights) + bias)\n",
    "print(test_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, criterion, max_depth):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Calculate impurity measure\n",
    "        if self.criterion == 'gini':\n",
    "            self.impurity = self._gini\n",
    "        elif self.criterion == 'entropy':\n",
    "            self.impurity = self._entropy\n",
    "        else:\n",
    "            raise ValueError(\"Invalid criterion\")\n",
    "        \n",
    "        # Find the best question and split the data accordingly\n",
    "        self.tree = self._build_tree(X, y, depth=1)\n",
    "    \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # Count number of samples and unique labels\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_labels = np.unique(y)\n",
    "        \n",
    "        # Calculate initial impurity of the data\n",
    "        initial_impurity = self.impurity(y)\n",
    "        \n",
    "        # Initialize variables to keep track of the best split\n",
    "        best_gain = 0\n",
    "        best_question = None\n",
    "        best_groups = None\n",
    "        \n",
    "        # Loop through all features and values to find the best split\n",
    "        for feature_index in range(num_features):\n",
    "            # Get values of the current feature\n",
    "            feature_values = X[:, feature_index]\n",
    "            \n",
    "            # Loop through unique values of the feature\n",
    "            for feature_value in np.unique(feature_values):\n",
    "                # Create the question for this feature and value\n",
    "                question = Question(feature_index, feature_value)\n",
    "                \n",
    "                # Split the data according to the question\n",
    "                groups = self._split(X, y, question)\n",
    "                \n",
    "                # Calculate the impurity gain from the split\n",
    "                impurity_gain = self._impurity_gain(groups, initial_impurity)\n",
    "                \n",
    "                # Update the best split if necessary\n",
    "                if impurity_gain > best_gain:\n",
    "                    best_gain = impurity_gain\n",
    "                    best_question = question\n",
    "                    best_groups = groups\n",
    "        \n",
    "        # Stop if no further improvement can be made or if max depth is reached\n",
    "        if best_gain == 0 or depth == self.max_depth:\n",
    "            return Leaf(y)\n",
    "        \n",
    "        # Recursively build the left and right branches of the tree\n",
    "        left_branch = self._build_tree(best_groups[0], best_groups[1], depth + 1)\n",
    "        right_branch = self._build_tree(best_groups[2], best_groups[3], depth + 1)\n",
    "        \n",
    "        # Return a node with the best question and the left and right branches\n",
    "        return Node(best_question, left_branch, right_branch)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Traverse the tree and make predictions\n",
    "        return self._traverse_tree(X, self.tree)\n",
    "    \n",
    "    def _traverse_tree(self, X, node):\n",
    "        # Base case: reached a leaf node\n",
    "        if isinstance(node, Leaf):\n",
    "            return node.predictions\n",
    "        \n",
    "        # Decide whether to go left or right based on the question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # calculate mean, var, and prior for each class\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_c = X[y == c]\n",
    "            self._mean[idx, :] = X_c.mean(axis=0)\n",
    "            self._var[idx, :] = X_c.var(axis=0)\n",
    "            self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "            \n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        # calculate posterior probability for each class\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[idx])\n",
    "            posterior = np.sum(np.log(self._pdf(idx, x)))\n",
    "            posterior = posterior + prior\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # return class with the highest posterior\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _pdf(self, class_idx, x):\n",
    "        mean = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "def KNN(train_data, test_data, K):\n",
    "    distances = []\n",
    "    targets = []\n",
    "\n",
    "    # Tính khoảng cách euclidean giữa mỗi điểm dữ liệu trong tập huấn luyện và điểm dữ liệu trong tập kiểm tra\n",
    "    for i in range(len(train_data)):\n",
    "        distance = euclidean_distance(test_data, train_data[i][:-1])\n",
    "        distances.append([distance, i])\n",
    "    \n",
    "    # Sắp xếp các khoảng cách theo thứ tự tăng dần\n",
    "    distances = sorted(distances)\n",
    "\n",
    "    # Lấy nhãn của K điểm dữ liệu gần nhất trong tập huấn luyện\n",
    "    for i in range(K):\n",
    "        index = distances[i][1]\n",
    "        targets.append(train_data[index][-1])\n",
    "\n",
    "    # Trả về nhãn dự đoán cho tập kiểm tra dựa trên K điểm dữ liệu gần nhất\n",
    "    return max(set(targets), key = targets.count)\n",
    "\n",
    "# Đọc dữ liệu từ tệp csv và chuyển dữ liệu sang dạng numpy array\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data = data.values\n",
    "\n",
    "# Tách dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "train_data = data[:int(0.8 * len(data))]\n",
    "test_data = data[int(0.8 * len(data)):]\n",
    "\n",
    "# Lấy giá trị K\n",
    "K = 5\n",
    "\n",
    "# Dự đoán nhãn cho tập kiểm tra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1-x2)**2))\n",
    "\n",
    "class KMeans:\n",
    "\n",
    "    def __init__(self, K=5, max_iters=100, plot_steps=False):\n",
    "        self.K = K\n",
    "        self.max_iters = max_iters\n",
    "        self.plot_steps = plot_steps\n",
    "\n",
    "        # list of sample indices for each cluster\n",
    "        self.clusters = [[] for _ in range(self.K)]\n",
    "\n",
    "        # the centers (mean vector) for each cluster\n",
    "        self.centroids = []\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X = X\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "\n",
    "        # initialize\n",
    "        random_sample_idxs = np.random.choice(self.n_samples, self.K, replace=False)\n",
    "        self.centroids = [self.X[idx] for idx in random_sample_idxs]\n",
    "\n",
    "        # optimize clusters\n",
    "        for _ in range(self.max_iters):\n",
    "            # assign samples to closest centroids (create clusters)\n",
    "            self.clusters = self._create_clusters(self.centroids)\n",
    "\n",
    "            if self.plot_steps:\n",
    "                self.plot()\n",
    "\n",
    "            # calculate new centroids from the clusters\n",
    "            centroids_old = self.centroids\n",
    "            self.centroids = self._get_centroids(self.clusters)\n",
    "\n",
    "            if self._is_converged(centroids_old, self.centroids):\n",
    "                break\n",
    "\n",
    "            if self.plot_steps:\n",
    "                self.plot()\n",
    "\n",
    "        # classify samples as the index of their clusters\n",
    "        return self._get_cluster_labels(self.clusters)\n",
    "\n",
    "\n",
    "    def _get_cluster_labels(self, clusters):\n",
    "        # each sample will get the label of the cluster it was assigned to\n",
    "        labels = np.empty(self.n_samples)\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            for sample_idx in cluster:\n",
    "                labels[sample_idx] = cluster_idx\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "    def _create_clusters(self, centroids):\n",
    "        # assign the samples to the closest centroids\n",
    "        clusters = [[] for _ in range(self.K)]\n",
    "        for idx, sample in enumerate(self.X):\n",
    "            centroid_idx = self._closest_centroid(sample, centroids)\n",
    "            clusters[centroid_idx].append(idx)\n",
    "        return clusters\n",
    "\n",
    "    def _closest_centroid(self, sample, centroids):\n",
    "        # distance of the current sample to each centroid\n",
    "        distances = [euclidean_distance(sample, point) for point in centroids]\n",
    "        closest_idx = np.argmin(distances)\n",
    "        return closest_idx\n",
    "\n",
    "\n",
    "    def _get_centroids(self, clusters):\n",
    "        # assign mean value of clusters to centroids\n",
    "        centroids = np.zeros((self.K, self.n_features))\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            cluster_mean = np.mean(self.X[cluster], axis=0)\n",
    "            centroids[cluster_idx] = cluster_mean\n",
    "        return centroids\n",
    "\n",
    "    def _is_converged(self, centroids_old, centroids):\n",
    "        # distances between old and new centroids, for all centroids\n",
    "        distances = [euclidean_distance(centroids_old[i], centroids[i]) for i in range(self.K)]\n",
    "        return sum(distances) == 0\n",
    "\n",
    "    def plot(self):\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        for i, index in enumerate(self.clusters):\n",
    "            point = self.X[index].T\n",
    "            ax.scatter(*point)\n",
    "\n",
    "        for point in self.centroids:\n",
    "            ax.scatter(*point, marker=\"x\", color=\"black\", linewidth=2)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class PCA:\n",
    "\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # mean centering\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X = X -  self.mean\n",
    "\n",
    "        # covariance, functions needs samples as columns\n",
    "        cov = np.cov(X.T)\n",
    "\n",
    "        # eigenvectors, eigenvalues\n",
    "        eigenvectors, eigenvalues = np.linalg.eig(cov)\n",
    "\n",
    "        # eigenvectors v = [:, i] column vector, transpose this for easier calculations\n",
    "        eigenvectors = eigenvectors.T\n",
    "\n",
    "        # sort eigenvectors\n",
    "        idxs = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idxs]\n",
    "        eigenvectors = eigenvectors[idxs]\n",
    "\n",
    "        self.components = eigenvectors[:self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        # projects data\n",
    "        X = X - self.mean\n",
    "        return np.dot(X, self.components.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a44991a4104b2e7a0664df2d369b6eba42ee6000fe19a9a6e55b008932247e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
